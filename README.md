# üß© **LLM Loader Component ‚Äî LLMOps Medical Chatbot**

This branch introduces the **LLM loader component** for the LLMOps Medical Chatbot.
It adds the ability to initialise a Groq-hosted LLM using the `ChatGroq` interface, enabling fast, low-latency inference for medical question-answering using models such as **LLaMA 3.1**.

## üóÇÔ∏è **Project Structure (Updated)**

```text
LLMOPS-MEDICAL-CHATBOT/
‚îú‚îÄ‚îÄ .venv/
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ .python-version
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ setup.py
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ The_GALE_ENCYCLOPEDIA_OF_MEDICINE_SECOND.pdf
‚îÇ
‚îî‚îÄ‚îÄ app/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ
    ‚îú‚îÄ‚îÄ common/
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îú‚îÄ‚îÄ custom_exception.py
    ‚îÇ   ‚îú‚îÄ‚îÄ logger.py
    ‚îÇ   ‚îî‚îÄ‚îÄ README.md
    ‚îÇ
    ‚îú‚îÄ‚îÄ config/
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îú‚îÄ‚îÄ config.py
    ‚îÇ   ‚îî‚îÄ‚îÄ README.md
    ‚îÇ
    ‚îú‚îÄ‚îÄ components/
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îú‚îÄ‚îÄ pdf_loader.py
    ‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py
    ‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py
    ‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py
    ‚îÇ   ‚îî‚îÄ‚îÄ llm.py                # NEW: Loads Groq-hosted LLMs for inference
    ‚îÇ
    ‚îî‚îÄ‚îÄ templates/
```

> üí° The `.env` file must remain private, as it contains the `GROQ_API_KEY` used to authenticate with Groq's LLM API.

## ‚öôÔ∏è **What Was Done in This Branch**

1. **Added the `llm.py` component**

   * Implemented `load_llm()` to initialise a Groq-backed LLM via the LangChain v1 `ChatGroq` wrapper.
   * Configured sensible defaults (`llama-3.1-8b-instant`, `temperature=0.3`, `max_tokens=256`).
   * Integrated logging for full visibility into model loading steps.
   * Added robust exception handling using `CustomException`.

2. **Aligned all imports with the LangChain v1 ecosystem**

   * Adopted the `langchain_groq` package for Groq model loading.
   * Ensured compatibility with the project's existing LangChain v1 components.

3. **Applied full project-wide formatting**

   * File-level documentation
   * NumPy-style docstrings
   * Type hints
   * Section comment blocks
   * Clear explanatory inline comments

4. **Integrated seamlessly with the pipeline**

   * The loaded LLM will be used in the next stage: constructing a retrieval-augmented query answering module.

## üß™ **LLM Loader Status**

This component is now fully implemented and ready for use during the query-answering stage of the chatbot.
Model loading is logged clearly and includes error tracing through the custom exception system.

No runtime output is included here because the `llm.py` component does not execute a pipeline‚Äîits behaviour depends on downstream usage.

## ‚úÖ **Summary**

This branch introduces the Medical Chatbot‚Äôs inference layer:

* Groq-hosted LLM loading via LangChain v1
* Clean, modular component ready for integration
* Robust logging and exception management
* Completes the core components needed before building the retrieval + LLM answer generation pipeline
