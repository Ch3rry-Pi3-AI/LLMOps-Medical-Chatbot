# üîç **RAG Retriever Component ‚Äî LLMOps Medical Chatbot**

This branch introduces the **retriever component** for the LLMOps Medical Chatbot.
It implements the complete Retrieval-Augmented Generation (RAG) chain using the **LangChain v1 Expression Language (LCEL)**, replacing all deprecated `langchain.chains` APIs with modern runnable-based composition.

The RAG chain ties together the FAISS vector store retriever, the Groq-hosted LLM, and a custom medical safety-focused prompt to produce concise, context-grounded medical answers.

## üóÇÔ∏è **Project Structure (Updated)**

```text
LLMOPS-MEDICAL-CHATBOT/
‚îú‚îÄ‚îÄ .venv/
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ .python-version
‚îú‚îÄ‚îÄ pyproject.toml
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ setup.py
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ The_GALE_ENCYCLOPEDIA_OF_MEDICINE_SECOND.pdf
‚îÇ
‚îî‚îÄ‚îÄ app/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ
    ‚îú‚îÄ‚îÄ common/
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îú‚îÄ‚îÄ custom_exception.py
    ‚îÇ   ‚îú‚îÄ‚îÄ logger.py
    ‚îÇ   ‚îî‚îÄ‚îÄ README.md
    ‚îÇ
    ‚îú‚îÄ‚îÄ config/
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îú‚îÄ‚îÄ config.py
    ‚îÇ   ‚îî‚îÄ‚îÄ README.md
    ‚îÇ
    ‚îú‚îÄ‚îÄ components/
    ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
    ‚îÇ   ‚îú‚îÄ‚îÄ pdf_loader.py
    ‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py
    ‚îÇ   ‚îú‚îÄ‚îÄ vector_store.py
    ‚îÇ   ‚îú‚îÄ‚îÄ data_loader.py
    ‚îÇ   ‚îú‚îÄ‚îÄ llm.py
    ‚îÇ   ‚îî‚îÄ‚îÄ retriever.py            # NEW: Builds the LCEL RAG retrieval + LLM pipeline
    ‚îÇ
    ‚îî‚îÄ‚îÄ templates/
```

> üí° The `.env` file must remain private, as it contains the `GROQ_API_KEY` used to authenticate with Groq‚Äôs LLM API.

## ‚öôÔ∏è **What Was Done in This Branch**

1. **Added the `retriever.py` component**

   * Implemented a fully LangChain v1-compliant RAG pipeline.
   * Replaced all deprecated `langchain.chains` functionality with:

     * `RunnablePassthrough`
     * LCEL dictionary routing
     * `ChatPromptTemplate`
     * `StrOutputParser`
   * Constructed a clean LCEL pipeline:

     ```
     {
         "context": retriever,
         "question": RunnablePassthrough(),
     }
     | prompt
     | llm
     | StrOutputParser()
     ```

2. **Created a custom medical prompt**

   * Ensures context-grounding only.
   * Limits answers to 2‚Äì3 lines.
   * Instructs the model not to hallucinate.

3. **Integrated all core components**

   * Loads FAISS vector store via `vector_store.py`
   * Loads Groq LLM via `llm.py`
   * Produces a reusable, callable RAG chain for downstream inference.

4. **Applied full project-standard formatting**

   * File-level documentation
   * NumPy-style function docstrings
   * Type hints
   * Section comment blocks
   * Clear, intuitive inline comments

## üß™ **RAG Chain Status**

The LCEL RAG chain builds successfully and returns a runnable that accepts a user question and outputs a final, parsed medical answer.

All deprecated `langchain.chains` imports have been fully removed.

## ‚úÖ **Summary**

This branch completes the Medical Chatbot‚Äôs retrieval-and-reasoning layer:

* Full LangChain v1 LCEL RAG chain
* Safe, concise, context-only medical prompt
* Integration of retriever + LLM into a unified runnable
* Foundation for the final UI or API-based chatbot interface
